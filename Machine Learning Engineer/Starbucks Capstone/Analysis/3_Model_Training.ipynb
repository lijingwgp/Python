{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component objective\n",
    "\n",
    "Now that we have had a basic understanding of the collected customer population, and some common characteristics that can be employed to segment customers into multiple groups. We would like to all that previously generated results as our newly engineered features to let a machine learning model to learn off of. Furthermore, a simulation for the cost of sending the wrong offers is included to determine under what circumstances, or modeling errors, a trained model would produced the lowest cost for the business.\n",
    "\n",
    "Upon successful execution of this component, a trained model will be prepared for further predicting that which existing rewards program customers will likely to complete which type of offers next. \n",
    "\n",
    "Methodologies to be examined:\n",
    "- Logistic regression model\n",
    "- LightGBM classifier\n",
    "    - Bayesian optimization hyperparameter tuning\n",
    "    - Learning curve\n",
    "- Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. Data labeling, and final feature preparation\n",
    "2. Model training\n",
    "    - Logistic regression modeling as our baseline performance\n",
    "    - LightGBM classifier modeling as the potential better model\n",
    "    - Bayesian optimization based hyperparameter tuning\n",
    "    - Learning curve examination\n",
    "3. Cost simulation using previously made cost assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to answer:\n",
    "1. Should the marketing team send offer to a particular rewards program customer?\n",
    "2. If so, then which particular offer would help business achieve highest revenue and lowest cost?\n",
    "3. How does the customer groups labeled as ideal and valuable react to each type of offer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Final training data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data labeling\n",
    "Strategy:\n",
    "- To answer the first key questions above, we should rephrase it as \"which type of offers (informational, discount or bogo offers), a given customer is most likely to complete\". This way, we would be able to construct a multi-class learning objective and train a model of our choice.\n",
    "\n",
    "Labeling logic:\n",
    "0. A customer with a \"info_or_promo\" 0 indicates the following scenarios:\n",
    "    - This customer has not received any offers at all, hence labeled as unsuccessful.\n",
    "    - This customer has received informational offers (at least one), but failed to view them. Hence labeled as unsuccessful.\n",
    "    - This customer has received promotion offers (at least one), but failed to complete them. Hence labeled as unsuccessful.\n",
    "1. A customer with a \"info_or_promo\" 1 indicates this customer has received and viewed at least one informational offers.\n",
    "2. A customer with a \"info_or_promo\" 2 indicates this customer has received and completed at least one discount offers.\n",
    "3. A customer with a \"info_or_promo\" 3 indicates this customer has received and completed at least one bogo offers.\n",
    "\n",
    "Additional notes:\n",
    "- For an informational offer, success means viewing the offer; for a promotion offer, success means complete the offer.\n",
    "- Some customers may receive the same offer multiple times and they may or may not complete all of them. For this case, we only want to count when the customer completes such offer at least once. This is a modeling decision because when customer complete the same offer multiple times, it introduce duplicates to the dataset. And when the customer does not complete all of the same offers they receive, it introduce noise to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, learning_curve, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_curve, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import sample\n",
    "from numpy.random import uniform\n",
    "from math import isnan\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from lightgbm import LGBMClassifier, plotting\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/607991/Desktop/Starbucks Capstone/Data/Processed'\n",
    "profile = pd.read_csv(path+'/profile_processed2.csv')\n",
    "transcript = pd.read_csv(path+'/transcript_processed.csv')\n",
    "portfolio = pd.read_csv(path+'/portfolio_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data labeling for the first key question\n",
    "\n",
    "offer_received_df = transcript.query('event == \"offer received\"')\n",
    "offer_viewed_df = transcript.query('event == \"offer viewed\"')\n",
    "offer_completed_df = transcript.query('event==\"offer completed\"')\n",
    "\n",
    "info = ['3f207df678b143eea3cee63160fa8bed','5a8bc65990b245e5a138643cd4eb9837']\n",
    "bogo = ['ae264e3637204a6fb9bb56bc8210ddfd','4d5c57ea9a6940dd891ad53e9dbe8da0','9b98b8c7a33c4b65b9aebfe6a799e6d9',\n",
    "        'f19421c1d4aa40978ebb69ca19b0e20d']\n",
    "discount = ['0b1e1539f2cc45b7b9fa7c272da2e1d7','2298d6c36e964ae4a3e7e9706d1fb8c2','fafdcd668e3743c1bb461111dcafc2a4',\n",
    "           '2906b810c7d4411798c6938adc9daaa5']\n",
    "\n",
    "offer = []\n",
    "label = []\n",
    "person = []\n",
    "\n",
    "# looping through each customer\n",
    "for each_person in offer_received_df.person.unique().tolist():    \n",
    "\n",
    "    # for each customer, looping through each unique offer they received\n",
    "    for each_offer in offer_received_df.query('person == @each_person')['offer_id'].unique().tolist():    \n",
    "        \n",
    "        if (each_offer in info) and \\\n",
    "        (each_person in offer_viewed_df.query('offer_id == @each_offer').person.unique().tolist()):        # logic 1\n",
    "            label.append(1)\n",
    "            offer.append(each_offer)\n",
    "            person.append(each_person)\n",
    "        \n",
    "        elif (each_offer in discount) and \\\n",
    "        (each_person in offer_completed_df.query('offer_id == @each_offer').person.unique().tolist()):     # logic 2\n",
    "            label.append(2)\n",
    "            offer.append(each_offer)\n",
    "            person.append(each_person)\n",
    "        \n",
    "        elif (each_offer in bogo) and \\\n",
    "        (each_person in offer_completed_df.query('offer_id == @each_offer').person.unique().tolist()):     # logic 3\n",
    "            label.append(3)\n",
    "            offer.append(each_offer) \n",
    "            person.append(each_person)                                                                     \n",
    "            \n",
    "        else: \n",
    "            label.append(0)\n",
    "            offer.append(each_offer) \n",
    "            person.append(each_person)                                                                     # logic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the labels to the profile dataset which is our base dataset\n",
    "\n",
    "temp = pd.DataFrame(list(zip(person, offer, label)), columns=['person','offer','info_or_promo'])\n",
    "profile_new = pd.merge(profile.set_index('id'), temp, how='right', left_index=True, right_on='person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "      <th>amount</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>offer received</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53244</th>\n",
       "      <td>53244</td>\n",
       "      <td>offer received</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110837</th>\n",
       "      <td>110837</td>\n",
       "      <td>offer received</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150605</th>\n",
       "      <td>150605</td>\n",
       "      <td>offer received</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201579</th>\n",
       "      <td>201579</td>\n",
       "      <td>offer received</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245130</th>\n",
       "      <td>245130</td>\n",
       "      <td>offer received</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           event                            person  time  \\\n",
       "9                9  offer received  31dda685af34476cad5bc968bdb01c53     0   \n",
       "53244        53244  offer received  31dda685af34476cad5bc968bdb01c53   168   \n",
       "110837      110837  offer received  31dda685af34476cad5bc968bdb01c53   336   \n",
       "150605      150605  offer received  31dda685af34476cad5bc968bdb01c53   408   \n",
       "201579      201579  offer received  31dda685af34476cad5bc968bdb01c53   504   \n",
       "245130      245130  offer received  31dda685af34476cad5bc968bdb01c53   576   \n",
       "\n",
       "        amount                          offer_id  reward  \n",
       "9          NaN  0b1e1539f2cc45b7b9fa7c272da2e1d7     NaN  \n",
       "53244      NaN  fafdcd668e3743c1bb461111dcafc2a4     NaN  \n",
       "110837     NaN  2298d6c36e964ae4a3e7e9706d1fb8c2     NaN  \n",
       "150605     NaN  2298d6c36e964ae4a3e7e9706d1fb8c2     NaN  \n",
       "201579     NaN  2298d6c36e964ae4a3e7e9706d1fb8c2     NaN  \n",
       "245130     NaN  fafdcd668e3743c1bb461111dcafc2a4     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "      <th>amount</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96363</th>\n",
       "      <td>96363</td>\n",
       "      <td>offer completed</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143576</th>\n",
       "      <td>143576</td>\n",
       "      <td>offer completed</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174359</th>\n",
       "      <td>174359</td>\n",
       "      <td>offer completed</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230415</th>\n",
       "      <td>230415</td>\n",
       "      <td>offer completed</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0            event                            person  time  \\\n",
       "96363        96363  offer completed  31dda685af34476cad5bc968bdb01c53   258   \n",
       "143576      143576  offer completed  31dda685af34476cad5bc968bdb01c53   384   \n",
       "174359      174359  offer completed  31dda685af34476cad5bc968bdb01c53   426   \n",
       "230415      230415  offer completed  31dda685af34476cad5bc968bdb01c53   534   \n",
       "\n",
       "        amount                          offer_id  reward  \n",
       "96363      NaN  fafdcd668e3743c1bb461111dcafc2a4     2.0  \n",
       "143576     NaN  2298d6c36e964ae4a3e7e9706d1fb8c2     3.0  \n",
       "174359     NaN  2298d6c36e964ae4a3e7e9706d1fb8c2     3.0  \n",
       "230415     NaN  2298d6c36e964ae4a3e7e9706d1fb8c2     3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>age_group</th>\n",
       "      <th>became_member_year</th>\n",
       "      <th>became_member_month</th>\n",
       "      <th>became_member_day</th>\n",
       "      <th>became_member_weekday</th>\n",
       "      <th>offer_received</th>\n",
       "      <th>offer_viewed</th>\n",
       "      <th>offer_completed</th>\n",
       "      <th>offer_received_viewed_completed</th>\n",
       "      <th>offer_not_received_completed</th>\n",
       "      <th>offer_not_viewed_completed</th>\n",
       "      <th>amount_spent</th>\n",
       "      <th>reward_received</th>\n",
       "      <th>no_purchase</th>\n",
       "      <th>with_purchase</th>\n",
       "      <th>time</th>\n",
       "      <th>recency</th>\n",
       "      <th>r_quartile</th>\n",
       "      <th>f_quartile</th>\n",
       "      <th>m_quartile</th>\n",
       "      <th>rfm_score</th>\n",
       "      <th>rfm_score_sum</th>\n",
       "      <th>rfm_segments</th>\n",
       "      <th>clusters</th>\n",
       "      <th>person</th>\n",
       "      <th>offer</th>\n",
       "      <th>info_or_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.44</td>\n",
       "      <td>F</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>60-69</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099615</td>\n",
       "      <td>0.2</td>\n",
       "      <td>made_purchase</td>\n",
       "      <td>purchased_viewed</td>\n",
       "      <td>0.69863</td>\n",
       "      <td>0.30137</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>valuable</td>\n",
       "      <td>1</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.44</td>\n",
       "      <td>F</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>60-69</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099615</td>\n",
       "      <td>0.2</td>\n",
       "      <td>made_purchase</td>\n",
       "      <td>purchased_viewed</td>\n",
       "      <td>0.69863</td>\n",
       "      <td>0.30137</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>valuable</td>\n",
       "      <td>1</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.44</td>\n",
       "      <td>F</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>60-69</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099615</td>\n",
       "      <td>0.2</td>\n",
       "      <td>made_purchase</td>\n",
       "      <td>purchased_viewed</td>\n",
       "      <td>0.69863</td>\n",
       "      <td>0.30137</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>411</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>valuable</td>\n",
       "      <td>1</td>\n",
       "      <td>31dda685af34476cad5bc968bdb01c53</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age gender    income age_group  became_member_year  became_member_month  \\\n",
       "34  0.44      F  0.455556     60-69                 0.6             0.090909   \n",
       "35  0.44      F  0.455556     60-69                 0.6             0.090909   \n",
       "36  0.44      F  0.455556     60-69                 0.6             0.090909   \n",
       "\n",
       "    became_member_day  became_member_weekday  offer_received  offer_viewed  \\\n",
       "34           0.333333                    0.5             1.0      0.833333   \n",
       "35           0.333333                    0.5             1.0      0.833333   \n",
       "36           0.333333                    0.5             1.0      0.833333   \n",
       "\n",
       "    offer_completed  offer_received_viewed_completed  \\\n",
       "34         0.666667                         0.666667   \n",
       "35         0.666667                         0.666667   \n",
       "36         0.666667                         0.666667   \n",
       "\n",
       "    offer_not_received_completed  offer_not_viewed_completed  amount_spent  \\\n",
       "34                           0.0                         0.0      0.099615   \n",
       "35                           0.0                         0.0      0.099615   \n",
       "36                           0.0                         0.0      0.099615   \n",
       "\n",
       "    reward_received    no_purchase     with_purchase     time  recency  \\\n",
       "34              0.2  made_purchase  purchased_viewed  0.69863  0.30137   \n",
       "35              0.2  made_purchase  purchased_viewed  0.69863  0.30137   \n",
       "36              0.2  made_purchase  purchased_viewed  0.69863  0.30137   \n",
       "\n",
       "    r_quartile  f_quartile  m_quartile  rfm_score  rfm_score_sum rfm_segments  \\\n",
       "34           4           1           1        411       0.333333     valuable   \n",
       "35           4           1           1        411       0.333333     valuable   \n",
       "36           4           1           1        411       0.333333     valuable   \n",
       "\n",
       "    clusters                            person  \\\n",
       "34         1  31dda685af34476cad5bc968bdb01c53   \n",
       "35         1  31dda685af34476cad5bc968bdb01c53   \n",
       "36         1  31dda685af34476cad5bc968bdb01c53   \n",
       "\n",
       "                               offer  info_or_promo  \n",
       "34  0b1e1539f2cc45b7b9fa7c272da2e1d7              0  \n",
       "35  fafdcd668e3743c1bb461111dcafc2a4              2  \n",
       "36  2298d6c36e964ae4a3e7e9706d1fb8c2              2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examine labeling accuracy\n",
    "\n",
    "display(len(label) == len(person) == len(offer) == len(profile_new))\n",
    "display(transcript.query('person==\"31dda685af34476cad5bc968bdb01c53\" and event==\"offer received\"'))\n",
    "display(transcript.query('person==\"31dda685af34476cad5bc968bdb01c53\" and event==\"offer completed\"'))\n",
    "display(profile_new.query('person==\"31dda685af34476cad5bc968bdb01c53\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Taking customer \"31dda685af34476cad5bc968bdb01c53\" as an example, he or she received 6 offers in total, some offers have duplicates. In our resulting dataset, we only want to include 3 of these offers because out of all the offers, there are 3 unique ones.\n",
    "- This customer did not complete offer \"0b1e1539f2cc45b7b9fa7c272da2e1d7\", so we will give this register a \"0\" in the label column. However, this customer did complete discount offers \"fafdcd668e3743c1bb461111dcafc2a4\" and \"2298d6c36e964ae4a3e7e9706d1fb8c2\" (multiple times for this one), so we will label these register as \"2\"s in the label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Final Feature preparation\n",
    "\n",
    "- We drop no_purchase and with_purchase columns because they represent cumulated results that describes individual customers when the profile dataset was based on customer-level (each row vector represent one customer). So if we were to include these columns, then it wouldn't make sense because our label column is based on each purchase register. The no_purchase and with_purchase columns would contain values that are conflicts with the associated labeling values.\n",
    "- We also need to drop the columns that describe characteristics of each offers because we want to build a model that learn off of customers' characteristics and not to rely on the information about the offers. That said, we will not be joining with the portfolio dataset.\n",
    "- We keep the rfm_segments and clustering results because we assume that offers will be sending to the customers who are already members of the rewards program with some purchasing history. In other words, we assume that we wouldn't send to those who just joined our program with little or no purchasing history.\n",
    "    - This decision enables us to replicate the same processing steps (rfm analysis, k-prototype clustering) for the unseen customers and prepare them in a way so that the trained model can utilize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unwanted columns\n",
    "\n",
    "training_df = profile_new.drop(['offer', 'person', 'age_group', 'no_purchase','with_purchase'], axis=1)\n",
    "\n",
    "# convert certain columns to categorical using cat.code\n",
    "\n",
    "cat_cols = training_df.drop('info_or_promo', axis=1).select_dtypes(include=['object','uint16','int64']).columns.tolist()\n",
    "\n",
    "for each in cat_cols:\n",
    "    training_df[each] = training_df[each].astype('category')\n",
    "    training_df[each] = training_df[each].cat.codes\n",
    "    training_df[each] = training_df[each].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Final training data preparation conclusion\n",
    "\n",
    "Summary:\n",
    "- The entire first section of this component is designated to define a labeling strategy in which the resulted targets can be utilized by a machine learning model. We decided that the model learning objective would be predictiing multi-class labels. \n",
    "    - If a customer receives an informational offer and completed (viewed) it, then this instance would be labeled as class 1.\n",
    "    - If a customer receives a discount offer and completed it, then this instance would be labeled as class 2.\n",
    "    - If a customer receives a bogo offer and completed it, then this instance would be labeled as class 3.\n",
    "    - All else instances are labeled as class 0.\n",
    "- During the final feature praparation part, we intentionally left out features come from the portfolio dataset. Our goal is to train a model that only relies on customers' characteristics as well as their purchasing history.   \n",
    "- Please note that results (rfm_score, clusters) from the previous component remains in the final training set. This is because these results are derived from the characteristics of the customers and the analysis can be replicated in the future if we were given a set of unseen customers records to be learned in modeling training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Determining which type of offers to send (informational, discount or bogo offers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18066\n",
       "2    10993\n",
       "3     9785\n",
       "1     6723\n",
       "Name: info_or_promo, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train-test split\n",
    "\n",
    "train, test = train_test_split(training_df,test_size=0.2,random_state=0)\n",
    "train, val = train_test_split(train,test_size=0.1,random_state=0)\n",
    "x_train, x_test, x_val = train.drop('info_or_promo', axis=1), test.drop('info_or_promo', axis=1), val.drop('info_or_promo', axis=1)\n",
    "y_train, y_test, y_val = train['info_or_promo'], test['info_or_promo'], val['info_or_promo']\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use the \"class_weight\" option from the LogisticRegression LGBMClassifier models to counter class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3459  861  390  255]\n",
      " [ 686  555  337  290]\n",
      " [ 430  483 1390  808]\n",
      " [ 267  418  689 1340]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71      4965\n",
      "           1       0.24      0.30      0.27      1868\n",
      "           2       0.50      0.45      0.47      3111\n",
      "           3       0.50      0.49      0.50      2714\n",
      "\n",
      "    accuracy                           0.53     12658\n",
      "   macro avg       0.49      0.48      0.48     12658\n",
      "weighted avg       0.54      0.53      0.54     12658\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\607991\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model as our baseline model\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced', multi_class='multinomial')\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "y_pred_base = lr.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred_base))\n",
    "print(classification_report(y_test, y_pred_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's observed that the logistic regression model does not converge. This modeling result certainly is not usable as our baseline. For this reason, I will use the early-stopping option available in lightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\607991\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['clusters', 'f_quartile', 'gender', 'm_quartile', 'r_quartile', 'rfm_score', 'rfm_segments']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttraining's multi_logloss: 0.995605\tvalid_1's multi_logloss: 0.988739\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's multi_logloss: 0.987144\tvalid_1's multi_logloss: 0.987639\n",
      "[[3302  531  610  522]\n",
      " [ 613  483  359  413]\n",
      " [ 261  398 1508  944]\n",
      " [ 160  359  608 1587]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71      4965\n",
      "           1       0.27      0.26      0.27      1868\n",
      "           2       0.49      0.48      0.49      3111\n",
      "           3       0.46      0.58      0.51      2714\n",
      "\n",
      "    accuracy                           0.54     12658\n",
      "   macro avg       0.50      0.50      0.49     12658\n",
      "weighted avg       0.56      0.54      0.55     12658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LGBMClassifier baseline performance\n",
    "\n",
    "lgb = LGBMClassifier(objective='multiclass', metric='multi_logloss', class_weight='balanced')\n",
    "feature_name = x_train.columns.tolist()\n",
    "lgb.fit(x_train, y_train, eval_set = [(x_train, y_train), (x_val, y_val)], eval_metric= 'multi_logloss', \n",
    "        early_stopping_rounds=30, feature_name=feature_name, categorical_feature=cat_cols, verbose=50)\n",
    "\n",
    "y_pred = lgb.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model reached its best iteration before overfit. The lightGBM model does not perform well too for all class predictions. \n",
    "- We can conclude that the current train set does not contain meaningful information that allows a model to rely on in order to make correction predictions. \n",
    "- To solve this problem, I will add addtional information, about characteristics of different offers, from the portfolio dataset to the current train sets. Please note, that this move certainly will improve the modeling score, but we also take the risk of information leakage since different types of offers have fix set of channels, difficulty, and duration combinations.\n",
    "- So, it is my responsibility to find the right balance beween reveal too much information and not including enough of information. For this reason, I will only select the channels feature from the portfolio dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e016f39e80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting.plot_importance(lgb, max_num_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before moving onto joining the portfolio dataset, let's take a look at feature importance plot. We observe that the current lgb model learns most from rfm_score (our RFM segmentation results), offer_received, and reward_received. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'income', 'age_group', 'became_member_year',\n",
       "       'became_member_month', 'became_member_day', 'became_member_weekday',\n",
       "       'offer_received', 'offer_viewed', 'offer_completed',\n",
       "       'offer_received_viewed_completed', 'offer_not_received_completed',\n",
       "       'offer_not_viewed_completed', 'amount_spent', 'reward_received',\n",
       "       'no_purchase', 'with_purchase', 'time', 'recency', 'r_quartile',\n",
       "       'f_quartile', 'm_quartile', 'rfm_score', 'rfm_score_sum',\n",
       "       'rfm_segments', 'clusters', 'person', 'info_or_promo', 'channels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join with the portfolio dataset\n",
    "\n",
    "profile_new_2 = profile_new.set_index('offer').join(portfolio.set_index('id')['channels'])\n",
    "profile_new_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the same feature preparation as before\n",
    "\n",
    "training_df_2 = profile_new_2.drop(['person', 'age_group', 'no_purchase','with_purchase'], axis=1)\n",
    "cat_cols_2 = training_df_2.drop('info_or_promo', axis=1).select_dtypes(include=['object','uint16','int64']).columns.tolist()\n",
    "\n",
    "for each in cat_cols_2:\n",
    "    training_df_2[each] = training_df_2[each].astype('category')\n",
    "    training_df_2[each] = training_df_2[each].cat.codes\n",
    "    training_df_2[each] = training_df_2[each].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do another round of train-test split\n",
    "\n",
    "train_2, test_2 = train_test_split(training_df_2,test_size=0.2,random_state=0)\n",
    "train_2, val_2 = train_test_split(train_2,test_size=0.1,random_state=0)\n",
    "x_train_2, x_test_2, x_val_2 = train_2.drop('info_or_promo', axis=1), test_2.drop('info_or_promo', axis=1), val_2.drop('info_or_promo', axis=1)\n",
    "y_train_2, y_test_2, y_val_2 = train_2['info_or_promo'], test_2['info_or_promo'], val_2['info_or_promo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\607991\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['channels', 'clusters', 'f_quartile', 'gender', 'm_quartile', 'r_quartile', 'rfm_score', 'rfm_segments']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttraining's multi_logloss: 0.678037\tvalid_1's multi_logloss: 0.726407\n",
      "[100]\ttraining's multi_logloss: 0.634372\tvalid_1's multi_logloss: 0.714433\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's multi_logloss: 0.634372\tvalid_1's multi_logloss: 0.714433\n",
      "[[2640 1109  775  494]\n",
      " [ 115 1445   39  246]\n",
      " [  69  212 2174  652]\n",
      " [ 115  426  555 1592]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.53      0.66      5018\n",
      "           1       0.45      0.78      0.57      1845\n",
      "           2       0.61      0.70      0.65      3107\n",
      "           3       0.53      0.59      0.56      2688\n",
      "\n",
      "    accuracy                           0.62     12658\n",
      "   macro avg       0.62      0.65      0.61     12658\n",
      "weighted avg       0.69      0.62      0.63     12658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do another round of lgb training using the new train set\n",
    "\n",
    "lgb_2 = LGBMClassifier(objective='multiclass', metric='multi_logloss', class_weight='balanced')\n",
    "feature_name = x_train_2.columns.tolist()\n",
    "lgb_2.fit(x_train_2, y_train_2, eval_set = [(x_train_2, y_train_2), (x_val_2, y_val_2)], \n",
    "        eval_metric= 'multi_logloss', early_stopping_rounds=30, feature_name=feature_name, \n",
    "        categorical_feature=cat_cols_2, verbose=50)\n",
    "\n",
    "y_pred_2 = lgb_2.predict(x_test_2)\n",
    "print(confusion_matrix(y_test_2, y_pred_2))\n",
    "print(classification_report(y_test_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's observed that the modeling score, in terms of weighted avg f1, has been improved from 0.55 to 0.63.\n",
    "- Including the new feature \"channels\" from the portfolio dataset is the maximum level of tolerance I have towards training features. Next, I will tune the model and use its learning curve to determine whether the model has reached its potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model improvements - Bayesian optimization based parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter search space\n",
    "\n",
    "lgb_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 3, 20, 1)),\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 200, 800, 1)),\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 10, 100, 1)),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.6, 1.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 20.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 20.0),\n",
    "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 100, 1)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization based model parameter tuning\n",
    "\n",
    "def hyperopt(estimator, param_space, X_train, y_train, X_test, y_test, num_eval, objective=None, eval_metric=None, \n",
    "             fit_metric=None, class_weight=None):\n",
    "    \n",
    "    start = time.time()\n",
    "    def objective_function(params):\n",
    "        model = estimator(**params, objective=objective, class_weight=class_weight)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=5, scoring=eval_metric).mean()\n",
    "        return {'loss': -score, 'status': STATUS_OK}\n",
    "    \n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective_function, \n",
    "                      param_space, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate= np.random.RandomState(1))\n",
    "    \n",
    "    loss = [x['result']['loss'] for x in trials.trials]\n",
    "    feature_name = X_train.columns.tolist()\n",
    "    cat_cols = X_train.select_dtypes(include='category').columns.tolist()\n",
    "    \n",
    "    \n",
    "    if str(estimator) == \"<class 'lightgbm.sklearn.LGBMClassifier'>\":\n",
    "        \n",
    "        for each in ['num_leaves','max_depth','n_estimators','min_child_samples']:\n",
    "            best_param[each] = int(best_param[each])\n",
    "        \n",
    "        model_best = estimator(**best_param, objective=objective, class_weight=class_weight)     \n",
    "        model_best.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], eval_metric= fit_metric, \n",
    "                       early_stopping_rounds=30, feature_name=feature_name, categorical_feature=cat_cols_2, verbose=100)\n",
    "        y_pred = model_best.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"##### Results #####\")\n",
    "    print(\"Score best parameters: \", min(loss)*-1)\n",
    "    print(\"Best parameters: \", best_param)\n",
    "    print(\"Test Score: \", f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"Time elapsed: \", time.time() - start)\n",
    "    print(\"Parameter combinations evaluated: \", num_eval)\n",
    "    \n",
    "    \n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 30/30 [13:11<00:00, 26.38s/it, best loss: -0.6308399599445543]\n",
      "Training until validation scores don't improve for 30 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\607991\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['channels', 'clusters', 'f_quartile', 'gender', 'm_quartile', 'r_quartile', 'rfm_score', 'rfm_segments']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\ttraining's multi_logloss: 0.899249\tvalid_1's multi_logloss: 0.91948\n",
      "[200]\ttraining's multi_logloss: 0.762402\tvalid_1's multi_logloss: 0.797495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[287]\ttraining's multi_logloss: 0.710297\tvalid_1's multi_logloss: 0.755336\n",
      "\n",
      "##### Results #####\n",
      "Score best parameters:  0.6308399599445543\n",
      "Best parameters:  {'bagging_fraction': 0.7915826310397323, 'colsample_by_tree': 0.7022779271309514, 'learning_rate': 0.012570946395584776, 'max_depth': 18, 'min_child_samples': 54, 'n_estimators': 287, 'num_leaves': 44, 'reg_alpha': 1.2330346362844646, 'reg_lambda': 14.052899848887177}\n",
      "Test Score:  0.626379371477563\n",
      "Time elapsed:  797.4922697544098\n",
      "Parameter combinations evaluated:  30\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "num_eval=30\n",
    "eval_metric = 'f1_weighted'\n",
    "class_weight = 'balanced'\n",
    "objective = 'multiclass'\n",
    "fit_metric = 'multi_logloss'\n",
    "\n",
    "lgb_hyperopt = hyperopt(LGBMClassifier, lgb_space, x_train_2, y_train_2, x_val_2, y_val_2, num_eval, objective, \n",
    "                        eval_metric, fit_metric, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the best parameters selected\n",
    "\n",
    "unpack_function = lambda l: [item for sublist in l for item in sublist]\n",
    "unpack_all = lgb_hyperopt.best_trial['misc']['vals']\n",
    "unpack_values = [i for i in unpack_all.values()]\n",
    "values = unpack_function(unpack_values)\n",
    "keys = [i for i in unpack_all.keys()]\n",
    "best_param = {keys[i]: values[i] for i in range(len(keys))} \n",
    "\n",
    "for each in ['num_leaves','max_depth','n_estimators','min_child_samples']:\n",
    "    best_param[each] = int(best_param[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\607991\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['channels', 'clusters', 'f_quartile', 'gender', 'm_quartile', 'r_quartile', 'rfm_score', 'rfm_segments']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.899249\tvalid_1's multi_logloss: 0.91948\n",
      "[200]\ttraining's multi_logloss: 0.762402\tvalid_1's multi_logloss: 0.797495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[287]\ttraining's multi_logloss: 0.710297\tvalid_1's multi_logloss: 0.755336\n",
      "[[2594 1143  791  490]\n",
      " [ 120 1456   35  234]\n",
      " [  54  211 2177  665]\n",
      " [  86  417  503 1682]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.52      0.66      5018\n",
      "           1       0.45      0.79      0.57      1845\n",
      "           2       0.62      0.70      0.66      3107\n",
      "           3       0.55      0.63      0.58      2688\n",
      "\n",
      "    accuracy                           0.62     12658\n",
      "   macro avg       0.63      0.66      0.62     12658\n",
      "weighted avg       0.69      0.62      0.63     12658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_final = LGBMClassifier(objective='multiclass', metric='multi_logloss', class_weight='balanced', **best_param)\n",
    "feature_name = x_train_2.columns.tolist()\n",
    "lgb_final.fit(x_train_2, y_train_2, eval_set = [(x_train_2, y_train_2), (x_val_2, y_val_2)], eval_metric= 'multi_logloss', \n",
    "        early_stopping_rounds=30, feature_name=feature_name, categorical_feature=cat_cols_2, verbose=100)\n",
    "\n",
    "y_pred_final = lgb_final.predict(x_test_2)\n",
    "print(confusion_matrix(y_test_2, y_pred_final))\n",
    "print(classification_report(y_test_2, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter did not improve model's performance. This indicates that there is a fix amount of information can be learned by the model and that information mainly comes from the channels column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Modeling performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "\n",
    "def learning_curve_graph(model, x, y, scorer, tune_params, tune_result, title, x_label, y_label, \n",
    "                         objective=None, metric=None, class_weight=None):\n",
    "    \n",
    "    # Initialize a learning curve figure\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8, 7))\n",
    "    axes.set_title(title)\n",
    "    axes.set_ylim(tune_result[0], tune_result[1])\n",
    "    axes.set_xlabel(x_label)\n",
    "    axes.set_ylabel(y_label)\n",
    "\n",
    "    # learning curve calculations\n",
    "\n",
    "    cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "    learning_curve(model(**tune_params, objective=objective, metric=metric, class_weight=class_weight), \n",
    "                   x, y, scoring=scorer,\n",
    "                   cv=cv, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot the curve\n",
    "\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                      color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                      color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"b\", label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objective='multiclass'\n",
    "metric='multi_logloss'\n",
    "class_weight='balanced'\n",
    "result_range = [(lgb_hyperopt.best_trial['result']['loss']*-1)-0.1, (lgb_hyperopt.best_trial['result']['loss']*-1)+0.1]\n",
    "title = \"Learning Curves (LGBMClassifier)\"\n",
    "x_label = \"Training examples\"\n",
    "y_label = \"F1_weighted score\"\n",
    "\n",
    "learning_curve_graph(LGBMClassifier, x_train_2, y_train_2, 'f1_weighted', best_param, result_range, title, x_label, y_label,\n",
    "                    objective, metric, class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We observe that when the trainig size is small, the gap test learning curve and train learning curve is large. This is expected as the model could just memorize all the answers given that there are only a few training samples. \n",
    "- However, as the training size became larger and larger, the gap between train learning curve and test learning curve narrows down. This trend indicates that the model became more and more stable as more training samples are introduced. In addition, there is no sign of overfitting, and it is very likely that the training error and test error came across at around 0.63."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model training conclusion\n",
    "Summary:\n",
    "- A logistic regression model, with class_weight option set as balanced, was first trained to establish baseline model performance, in terms of f1-weighted score. We observe that its test score was 0.54 which is almost equivalent to a dummy classifier.\n",
    "- Next, a lightGBM classifier was trained, and we observe that there is no performance gain even if lightGBM is considered as a potentially better learner in many cases. This result prompt us to further include more information from the portfolio dataset, namely the channels feature. \n",
    "- Please note that the portfolio dataset contains information that describes each offer's nature, and our modeling objective is to predict which type of offers (information, bogo, or discount) a given customer would most certainly complete. So if we were adding to much information from the portfolio dataset, we would have high risk of information leakage. The maximum level of tollerance towards adding in new features, would be only allowing the channels feature to be added to the training set.\n",
    "- As a result, a trained lightGBM classifier improves its test f1 weighted score to 0.63. But we also observe that there is no potential value presides in other features that describe customer characteristics. \n",
    "\n",
    "Conclusion:\n",
    "- We conclude that a trained lightGBM classifier model eventually will achieve 0.63 as the final testing f1 weighted score. This score has been improved from 0.55 when the channels column was not included. Since we only added the channels column as an extra feature and no sign of improvements are observed during parameter tuning, we conclude that the testing score improvements is mainly caused by the channels column, and there is only a fixed amount of information that a model can learn off of. This conclusion can be proven by observing from model's curve.\n",
    "- This scoring result is rather poor and it implies that the model should not be used in production until either more meaningful features are added or data labeling logics are refined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost simulation\n",
    "\n",
    "Original plan:\n",
    "\n",
    "Now that the first key question (which type of offers to send) is answered, we can move onto the second key question, \"which particular offer would help business achieve highest revenue and lowest cost?\". A common approach to solve this problem would be starting from a train model and analyze its error rates such as recall, and precision. By adjusting the probability threshold (only applicable in a binary problem), we would be able to observe how recall and precisions behaves accordingly. Then by applying the cost assumptions to various recall/precision, we would be able to obtain a cost function that allows us to determine at what probability threshold and recall/precision level, the cost of modeling errors could be minimized. \n",
    "\n",
    "New plan:\n",
    "\n",
    "However, since our modeling objective is mul-class prediction by design, the concept of adjusting the probability threshold will not be applicable in our case. Thus, I purposely choose to do a cost simulation instead of a cost analysis. The machanism that drives the simulation is that by adjusting predicted probabilities of each class, we would obtain various level of simulated modeling error rates for which we could then apply to the cost assumptions. As a result, different cost will be shown in addition to their associated error rates and other modeling performance metrics. We would then be able to pick a cost that's acceptable. In the subsequent model improvement works, we can aim to achieve that observed modeling performance, knowing that the cost would be minized as a result of this.\n",
    "\n",
    "New question to answer:\n",
    "\n",
    "Because of the decision made above, I will rephrase the second question which is stated in the begining of this analysis. Thus, instead of answering \"which particular offer would help business achieve highest revenue and lowest cost?\", I will use the following cost simulation to answer \"given the current state of the model, what improvements can be made in regards to modeling performance for each class label, to achieve a minimized cost after applying the cost assumptions?\".\n",
    "\n",
    "Additional note:\n",
    "- It's because predicted class probabilities are the main driver for various level of prevision and recall, we choose it as the sole input of the simulator function.\n",
    "- The current model performance is rather poor. This is another reason I decided to do simulations because it represents hypothetical cases to show a minimized cost that can be potentially achieved if model performance is improved to a certain level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost assumptions\n",
    "\n",
    "- Based on the entire customer population, which is unknown to us, the percentage of rewards program members who remain to be in-active (at-risk) is about 26.448%.\n",
    "- The bogo and discount offers cost the same, \\\\$1600 dollars per customer (one-time).\n",
    "- If a potentially interested customer is lost then it will cost the business \\\\$11500 dollars per customer (one-time).\n",
    "- The bogo and discount offers are assumed to be 45% effective on average. That is 45% of the rewards program members who are identified as potential participants and they have received the bogo and discount offers, actually decide to participate and make purchasing decisions. The remaining 55% will not participate anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted class probability simulator\n",
    "\n",
    "def probability_simulator(empty_list, y_true_prob, y_true, col_index):\n",
    "    \n",
    "    for each in np.linspace(y_true_prob[col_index].mean(), 0.99, 10):\n",
    "\n",
    "        mu, sigma = each, y_true_prob[col_index].std()\n",
    "        s = np.random.normal(mu, sigma, 12658)\n",
    "\n",
    "        class_prob_simulated = s[s<1]\n",
    "        class_prob_simulated = pd.DataFrame(resample(class_prob_simulated, replace=True, \n",
    "                                                     n_samples=12658, random_state=123), columns=[col_index])\n",
    "        y_true_prob[col_index] = class_prob_simulated\n",
    "        y_pred_simulated = y_true_prob.idxmax(axis=1)\n",
    "        cnf_matrix = confusion_matrix(y_true, y_pred_simulated)\n",
    "\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.astype(float)\n",
    "        FN = FN.astype(float)\n",
    "        TP = TP.astype(float)\n",
    "        TN = TN.astype(float)\n",
    "\n",
    "        # true positive rate\n",
    "        TPR = TP/(TP+FN)\n",
    "        # true negative rate\n",
    "        TNR = TN/(TN+FP) \n",
    "        # positive predictive value\n",
    "        PPV = TP/(TP+FP)\n",
    "        # negative predictive value\n",
    "        NPV = TN/(TN+FN)\n",
    "        # false positive rate\n",
    "        FPR = FP/(FP+TN)\n",
    "        # false negative rate\n",
    "        FNR = FN/(TP+FN)\n",
    "        # false discovery rate\n",
    "        FDR = FP/(TP+FP)\n",
    "        # overall accuracy\n",
    "        ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "        empty_list.append([mu, sigma, TPR[col_index], TNR[col_index], PPV[col_index], NPV[col_index], FPR[col_index], \n",
    "                        FNR[col_index], FDR[col_index], ACC[col_index]])\n",
    "\n",
    "    empty_list.append(['class1_prob_mean', 'class1_prob_std', 'tpr','tnr','ppv','npv','fpr','fnr','fdr','overall_accuracy'])\n",
    "    column_names = empty_list.pop(10)\n",
    "    df = pd.DataFrame(empty_list, columns=column_names)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# cost function\n",
    "\n",
    "def cost_simulator(df):\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,'expected_cost'] = (1-0.26448)*df.loc[i,'fpr']*1600 + \\\n",
    "        0.26448*df.loc[i,'fnr']*11500 + \\\n",
    "        0.26448*df.loc[i,'tpr']*1600*0.45 + \\\n",
    "        (1600+11500)*0.55*0.26448*df.loc[i,'tpr']\n",
    "    display(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1_prob_mean</th>\n",
       "      <th>class1_prob_std</th>\n",
       "      <th>tpr</th>\n",
       "      <th>tnr</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>fpr</th>\n",
       "      <th>fnr</th>\n",
       "      <th>fdr</th>\n",
       "      <th>overall_accuracy</th>\n",
       "      <th>expected_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.250789</td>\n",
       "      <td>0.372358</td>\n",
       "      <td>0.869139</td>\n",
       "      <td>0.326832</td>\n",
       "      <td>0.890299</td>\n",
       "      <td>0.130861</td>\n",
       "      <td>0.627642</td>\n",
       "      <td>0.673168</td>\n",
       "      <td>0.796729</td>\n",
       "      <td>2843.451228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300480</td>\n",
       "      <td>0.248649</td>\n",
       "      <td>0.497561</td>\n",
       "      <td>0.806252</td>\n",
       "      <td>0.304680</td>\n",
       "      <td>0.903888</td>\n",
       "      <td>0.193748</td>\n",
       "      <td>0.502439</td>\n",
       "      <td>0.695320</td>\n",
       "      <td>0.761258</td>\n",
       "      <td>2799.077296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.386670</td>\n",
       "      <td>0.245498</td>\n",
       "      <td>0.621680</td>\n",
       "      <td>0.728198</td>\n",
       "      <td>0.280715</td>\n",
       "      <td>0.918572</td>\n",
       "      <td>0.271802</td>\n",
       "      <td>0.378320</td>\n",
       "      <td>0.719285</td>\n",
       "      <td>0.712672</td>\n",
       "      <td>2773.577241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.472860</td>\n",
       "      <td>0.240031</td>\n",
       "      <td>0.727913</td>\n",
       "      <td>0.644872</td>\n",
       "      <td>0.259116</td>\n",
       "      <td>0.932843</td>\n",
       "      <td>0.355128</td>\n",
       "      <td>0.272087</td>\n",
       "      <td>0.740884</td>\n",
       "      <td>0.656976</td>\n",
       "      <td>2771.192444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.227986</td>\n",
       "      <td>0.805962</td>\n",
       "      <td>0.554055</td>\n",
       "      <td>0.235695</td>\n",
       "      <td>0.943613</td>\n",
       "      <td>0.445945</td>\n",
       "      <td>0.194038</td>\n",
       "      <td>0.764305</td>\n",
       "      <td>0.590773</td>\n",
       "      <td>2804.271966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.645240</td>\n",
       "      <td>0.215213</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.217218</td>\n",
       "      <td>0.959072</td>\n",
       "      <td>0.544900</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.782782</td>\n",
       "      <td>0.517933</td>\n",
       "      <td>2844.879061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.731430</td>\n",
       "      <td>0.196827</td>\n",
       "      <td>0.944173</td>\n",
       "      <td>0.357533</td>\n",
       "      <td>0.200483</td>\n",
       "      <td>0.974049</td>\n",
       "      <td>0.642467</td>\n",
       "      <td>0.055827</td>\n",
       "      <td>0.799517</td>\n",
       "      <td>0.443040</td>\n",
       "      <td>2904.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.817620</td>\n",
       "      <td>0.169806</td>\n",
       "      <td>0.983198</td>\n",
       "      <td>0.266069</td>\n",
       "      <td>0.186051</td>\n",
       "      <td>0.989340</td>\n",
       "      <td>0.733931</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.813949</td>\n",
       "      <td>0.370596</td>\n",
       "      <td>2975.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.903810</td>\n",
       "      <td>0.135958</td>\n",
       "      <td>0.991328</td>\n",
       "      <td>0.199297</td>\n",
       "      <td>0.174406</td>\n",
       "      <td>0.992630</td>\n",
       "      <td>0.800703</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>0.825594</td>\n",
       "      <td>0.314742</td>\n",
       "      <td>3046.496342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.099891</td>\n",
       "      <td>0.999458</td>\n",
       "      <td>0.128641</td>\n",
       "      <td>0.163678</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>0.871359</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.836322</td>\n",
       "      <td>0.255570</td>\n",
       "      <td>3121.959100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class1_prob_mean  class1_prob_std       tpr       tnr       ppv       npv  \\\n",
       "0          0.214289         0.250789  0.372358  0.869139  0.326832  0.890299   \n",
       "1          0.300480         0.248649  0.497561  0.806252  0.304680  0.903888   \n",
       "2          0.386670         0.245498  0.621680  0.728198  0.280715  0.918572   \n",
       "3          0.472860         0.240031  0.727913  0.644872  0.259116  0.932843   \n",
       "4          0.559050         0.227986  0.805962  0.554055  0.235695  0.943613   \n",
       "5          0.645240         0.215213  0.886179  0.455100  0.217218  0.959072   \n",
       "6          0.731430         0.196827  0.944173  0.357533  0.200483  0.974049   \n",
       "7          0.817620         0.169806  0.983198  0.266069  0.186051  0.989340   \n",
       "8          0.903810         0.135958  0.991328  0.199297  0.174406  0.992630   \n",
       "9          0.990000         0.099891  0.999458  0.128641  0.163678  0.999282   \n",
       "\n",
       "        fpr       fnr       fdr  overall_accuracy  expected_cost  \n",
       "0  0.130861  0.627642  0.673168          0.796729    2843.451228  \n",
       "1  0.193748  0.502439  0.695320          0.761258    2799.077296  \n",
       "2  0.271802  0.378320  0.719285          0.712672    2773.577241  \n",
       "3  0.355128  0.272087  0.740884          0.656976    2771.192444  \n",
       "4  0.445945  0.194038  0.764305          0.590773    2804.271966  \n",
       "5  0.544900  0.113821  0.782782          0.517933    2844.879061  \n",
       "6  0.642467  0.055827  0.799517          0.443040    2904.865100  \n",
       "7  0.733931  0.016802  0.813949          0.370596    2975.604651  \n",
       "8  0.800703  0.008672  0.825594          0.314742    3046.496342  \n",
       "9  0.871359  0.000542  0.836322          0.255570    3121.959100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost simulations for class 1 predictions\n",
    "\n",
    "y_prob = pd.DataFrame(lgb_final.predict_proba(x_test_2))\n",
    "temp = []\n",
    "class1_simulation_results = probability_simulator(temp, y_prob, y_test_2, 1)\n",
    "class1_simulation_results = cost_simulator(class1_simulation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final lightGBM classifer after tuned has a class 1 recall rate at 0.79. We observe from the simulation results, an optimal cost is achieved when class 1 recall rate is at 0.72. At first, this observation might seem contradicting to what our goal would be, which is to improve recall rate for all class predictions. \n",
    "- However, this observation can be interpreted as a potentially better model could sacrifice its performance in class 1 predictions in order for it to gain performance in other class predictions. This is a business decision to be made.\n",
    "- As the simulated results suggest, a recall at 0.72 would achieve an expected cost of \\\\$2771 dollars per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1_prob_mean</th>\n",
       "      <th>class1_prob_std</th>\n",
       "      <th>tpr</th>\n",
       "      <th>tnr</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>fpr</th>\n",
       "      <th>fnr</th>\n",
       "      <th>fdr</th>\n",
       "      <th>overall_accuracy</th>\n",
       "      <th>expected_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.266093</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.851848</td>\n",
       "      <td>0.472998</td>\n",
       "      <td>0.815803</td>\n",
       "      <td>0.148152</td>\n",
       "      <td>0.591246</td>\n",
       "      <td>0.527002</td>\n",
       "      <td>0.743087</td>\n",
       "      <td>2829.386196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333810</td>\n",
       "      <td>0.262135</td>\n",
       "      <td>0.514322</td>\n",
       "      <td>0.799916</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>0.835064</td>\n",
       "      <td>0.200084</td>\n",
       "      <td>0.485678</td>\n",
       "      <td>0.544600</td>\n",
       "      <td>0.729815</td>\n",
       "      <td>2790.684822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.415834</td>\n",
       "      <td>0.255603</td>\n",
       "      <td>0.614741</td>\n",
       "      <td>0.729034</td>\n",
       "      <td>0.424633</td>\n",
       "      <td>0.853309</td>\n",
       "      <td>0.270966</td>\n",
       "      <td>0.385259</td>\n",
       "      <td>0.575367</td>\n",
       "      <td>0.700980</td>\n",
       "      <td>2779.154556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497857</td>\n",
       "      <td>0.246860</td>\n",
       "      <td>0.723206</td>\n",
       "      <td>0.642969</td>\n",
       "      <td>0.397207</td>\n",
       "      <td>0.877160</td>\n",
       "      <td>0.357031</td>\n",
       "      <td>0.276794</td>\n",
       "      <td>0.602793</td>\n",
       "      <td>0.662664</td>\n",
       "      <td>2777.882599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.579881</td>\n",
       "      <td>0.233567</td>\n",
       "      <td>0.795945</td>\n",
       "      <td>0.549052</td>\n",
       "      <td>0.364749</td>\n",
       "      <td>0.892140</td>\n",
       "      <td>0.450948</td>\n",
       "      <td>0.204055</td>\n",
       "      <td>0.635251</td>\n",
       "      <td>0.609654</td>\n",
       "      <td>2819.631108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.661905</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>0.893145</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.350069</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.539420</td>\n",
       "      <td>0.106855</td>\n",
       "      <td>0.649931</td>\n",
       "      <td>0.566756</td>\n",
       "      <td>2831.844239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.743929</td>\n",
       "      <td>0.190274</td>\n",
       "      <td>0.959125</td>\n",
       "      <td>0.361114</td>\n",
       "      <td>0.328122</td>\n",
       "      <td>0.964485</td>\n",
       "      <td>0.638886</td>\n",
       "      <td>0.040875</td>\n",
       "      <td>0.671878</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>2886.513851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.825952</td>\n",
       "      <td>0.161465</td>\n",
       "      <td>0.983585</td>\n",
       "      <td>0.266569</td>\n",
       "      <td>0.303747</td>\n",
       "      <td>0.980362</td>\n",
       "      <td>0.733431</td>\n",
       "      <td>0.016415</td>\n",
       "      <td>0.696253</td>\n",
       "      <td>0.442566</td>\n",
       "      <td>2974.649352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.907976</td>\n",
       "      <td>0.130330</td>\n",
       "      <td>0.997425</td>\n",
       "      <td>0.182389</td>\n",
       "      <td>0.284103</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.817611</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.715897</td>\n",
       "      <td>0.382446</td>\n",
       "      <td>3060.629004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.095030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.122814</td>\n",
       "      <td>0.270527</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729473</td>\n",
       "      <td>0.338126</td>\n",
       "      <td>3128.304125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class1_prob_mean  class1_prob_std       tpr       tnr       ppv       npv  \\\n",
       "0          0.251786         0.266093  0.408754  0.851848  0.472998  0.815803   \n",
       "1          0.333810         0.262135  0.514322  0.799916  0.455400  0.835064   \n",
       "2          0.415834         0.255603  0.614741  0.729034  0.424633  0.853309   \n",
       "3          0.497857         0.246860  0.723206  0.642969  0.397207  0.877160   \n",
       "4          0.579881         0.233567  0.795945  0.549052  0.364749  0.892140   \n",
       "5          0.661905         0.216419  0.893145  0.460580  0.350069  0.929825   \n",
       "6          0.743929         0.190274  0.959125  0.361114  0.328122  0.964485   \n",
       "7          0.825952         0.161465  0.983585  0.266569  0.303747  0.980362   \n",
       "8          0.907976         0.130330  0.997425  0.182389  0.284103  0.995429   \n",
       "9          0.990000         0.095030  1.000000  0.122814  0.270527  1.000000   \n",
       "\n",
       "        fpr       fnr       fdr  overall_accuracy  expected_cost  \n",
       "0  0.148152  0.591246  0.527002          0.743087    2829.386196  \n",
       "1  0.200084  0.485678  0.544600          0.729815    2790.684822  \n",
       "2  0.270966  0.385259  0.575367          0.700980    2779.154556  \n",
       "3  0.357031  0.276794  0.602793          0.662664    2777.882599  \n",
       "4  0.450948  0.204055  0.635251          0.609654    2819.631108  \n",
       "5  0.539420  0.106855  0.649931          0.566756    2831.844239  \n",
       "6  0.638886  0.040875  0.671878          0.507900    2886.513851  \n",
       "7  0.733431  0.016415  0.696253          0.442566    2974.649352  \n",
       "8  0.817611  0.002575  0.715897          0.382446    3060.629004  \n",
       "9  0.877186  0.000000  0.729473          0.338126    3128.304125  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost simulations for class 2 predictions\n",
    "\n",
    "y_prob = pd.DataFrame(lgb_final.predict_proba(x_test_2))\n",
    "temp = []\n",
    "class2_simulation_results = probability_simulator(temp, y_prob, y_test_2, 2)\n",
    "class2_simulation_results = cost_simulator(class2_simulation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As the simulated results show, an optimal cost is achieved when the recall for class 2 predictions is at 0.72. \n",
    "- Our final model's recall, for class 2 predictions, is 0.70. So we interpret this as the current final model is already at its optimal state in regards to class 2 predicitons. \n",
    "- Using the stated cost assumption, it is reasonable to conclude that our final model has a expected cost of \\\\$2777 per customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1_prob_mean</th>\n",
       "      <th>class1_prob_std</th>\n",
       "      <th>tpr</th>\n",
       "      <th>tnr</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>fpr</th>\n",
       "      <th>fnr</th>\n",
       "      <th>fdr</th>\n",
       "      <th>overall_accuracy</th>\n",
       "      <th>expected_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.227124</td>\n",
       "      <td>0.227268</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>0.904814</td>\n",
       "      <td>0.465352</td>\n",
       "      <td>0.828907</td>\n",
       "      <td>0.095186</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.534648</td>\n",
       "      <td>0.777927</td>\n",
       "      <td>2862.988222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.311888</td>\n",
       "      <td>0.225584</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>0.841825</td>\n",
       "      <td>0.404681</td>\n",
       "      <td>0.838545</td>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.595319</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>2850.584055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.396652</td>\n",
       "      <td>0.224507</td>\n",
       "      <td>0.528646</td>\n",
       "      <td>0.770110</td>\n",
       "      <td>0.382709</td>\n",
       "      <td>0.858357</td>\n",
       "      <td>0.229890</td>\n",
       "      <td>0.471354</td>\n",
       "      <td>0.617291</td>\n",
       "      <td>0.718834</td>\n",
       "      <td>2812.218425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.481416</td>\n",
       "      <td>0.222630</td>\n",
       "      <td>0.658482</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0.370991</td>\n",
       "      <td>0.883606</td>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.341518</td>\n",
       "      <td>0.629009</td>\n",
       "      <td>0.690393</td>\n",
       "      <td>2773.144571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.566180</td>\n",
       "      <td>0.216452</td>\n",
       "      <td>0.777158</td>\n",
       "      <td>0.605015</td>\n",
       "      <td>0.346607</td>\n",
       "      <td>0.909667</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>0.222842</td>\n",
       "      <td>0.653393</td>\n",
       "      <td>0.641571</td>\n",
       "      <td>2771.535859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.203672</td>\n",
       "      <td>0.861979</td>\n",
       "      <td>0.501805</td>\n",
       "      <td>0.318094</td>\n",
       "      <td>0.930964</td>\n",
       "      <td>0.498195</td>\n",
       "      <td>0.138021</td>\n",
       "      <td>0.681906</td>\n",
       "      <td>0.578290</td>\n",
       "      <td>2812.796235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.735708</td>\n",
       "      <td>0.184863</td>\n",
       "      <td>0.937872</td>\n",
       "      <td>0.398696</td>\n",
       "      <td>0.296031</td>\n",
       "      <td>0.959681</td>\n",
       "      <td>0.601304</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>0.703969</td>\n",
       "      <td>0.513193</td>\n",
       "      <td>2862.380681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.820472</td>\n",
       "      <td>0.160236</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.303310</td>\n",
       "      <td>0.274796</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.696690</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.725204</td>\n",
       "      <td>0.446832</td>\n",
       "      <td>2935.589419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.905236</td>\n",
       "      <td>0.130555</td>\n",
       "      <td>0.997768</td>\n",
       "      <td>0.214042</td>\n",
       "      <td>0.254991</td>\n",
       "      <td>0.997196</td>\n",
       "      <td>0.785958</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.745009</td>\n",
       "      <td>0.380471</td>\n",
       "      <td>3023.054903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.096694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133701</td>\n",
       "      <td>0.237351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762649</td>\n",
       "      <td>0.317665</td>\n",
       "      <td>3115.492263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class1_prob_mean  class1_prob_std       tpr       tnr       ppv       npv  \\\n",
       "0          0.227124         0.227268  0.307292  0.904814  0.465352  0.828907   \n",
       "1          0.311888         0.225584  0.398810  0.841825  0.404681  0.838545   \n",
       "2          0.396652         0.224507  0.528646  0.770110  0.382709  0.858357   \n",
       "3          0.481416         0.222630  0.658482  0.698997  0.370991  0.883606   \n",
       "4          0.566180         0.216452  0.777158  0.605015  0.346607  0.909667   \n",
       "5          0.650944         0.203672  0.861979  0.501805  0.318094  0.930964   \n",
       "6          0.735708         0.184863  0.937872  0.398696  0.296031  0.959681   \n",
       "7          0.820472         0.160236  0.979167  0.303310  0.274796  0.981818   \n",
       "8          0.905236         0.130555  0.997768  0.214042  0.254991  0.997196   \n",
       "9          0.990000         0.096694  1.000000  0.133701  0.237351  1.000000   \n",
       "\n",
       "        fpr       fnr       fdr  overall_accuracy  expected_cost  \n",
       "0  0.095186  0.692708  0.534648          0.777927    2862.988222  \n",
       "1  0.158175  0.601190  0.595319          0.747748    2850.584055  \n",
       "2  0.229890  0.471354  0.617291          0.718834    2812.218425  \n",
       "3  0.301003  0.341518  0.629009          0.690393    2773.144571  \n",
       "4  0.394985  0.222842  0.653393          0.641571    2771.535859  \n",
       "5  0.498195  0.138021  0.681906          0.578290    2812.796235  \n",
       "6  0.601304  0.062128  0.703969          0.513193    2862.380681  \n",
       "7  0.696690  0.020833  0.725204          0.446832    2935.589419  \n",
       "8  0.785958  0.002232  0.745009          0.380471    3023.054903  \n",
       "9  0.866299  0.000000  0.762649          0.317665    3115.492263  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost simulations for class 3 predictions\n",
    "\n",
    "y_prob = pd.DataFrame(lgb_final.predict_proba(x_test_2))\n",
    "temp = []\n",
    "class3_simulation_results = probability_simulator(temp, y_prob, y_test_2, 3)\n",
    "class3_simulation_results = cost_simulator(class3_simulation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our final currently has a recall of 0.60 for class 3 predictions. However, as the simulation results suggest, modeling performance for class 3 predictions would our priority in the next iterations of modeling improvement works. \n",
    "- If we were able to achieve 0.77 for the class 3 recall, then the minized expected cost would be achieved at \\\\$2771 dollars per customer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cost simulation conclusion\n",
    "\n",
    "- Our current final model has a weighted avg f1 score of 0.63. By conducting simulations of various probabilities for each class, we were able to conclude that the area that future improvements is most needed is class 3 results. Specifically, we would like to achieve a 0.77 for class 3 recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of component 2 conclusion, I left a question unanswered, which is \"How does the customer groups labeled as ideal and valuable react to each type of offer?\". Now that we have joined the processed profile dataset (with RFM segment results appended) with the processed transcript dataset, I will answer this question first before moving onto the component 3 conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age_group</th>\n",
       "      <th>rfm_score</th>\n",
       "      <th>clusters</th>\n",
       "      <th>offer</th>\n",
       "      <th>info_or_promo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfm_segments</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ideal</th>\n",
       "      <td>F</td>\n",
       "      <td>50-59</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need_to_reach_out</th>\n",
       "      <td>M</td>\n",
       "      <td>50-59</td>\n",
       "      <td>344</td>\n",
       "      <td>3</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_spending_enough</th>\n",
       "      <td>M</td>\n",
       "      <td>50-59</td>\n",
       "      <td>444</td>\n",
       "      <td>3</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valuable</th>\n",
       "      <td>M</td>\n",
       "      <td>50-59</td>\n",
       "      <td>311</td>\n",
       "      <td>1</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    gender age_group  rfm_score  clusters  \\\n",
       "rfm_segments                                                \n",
       "ideal                    F     50-59        111         1   \n",
       "need_to_reach_out        M     50-59        344         3   \n",
       "not_spending_enough      M     50-59        444         3   \n",
       "valuable                 M     50-59        311         1   \n",
       "\n",
       "                                                offer  info_or_promo  \n",
       "rfm_segments                                                          \n",
       "ideal                fafdcd668e3743c1bb461111dcafc2a4              2  \n",
       "need_to_reach_out    3f207df678b143eea3cee63160fa8bed              0  \n",
       "not_spending_enough  3f207df678b143eea3cee63160fa8bed              0  \n",
       "valuable             fafdcd668e3743c1bb461111dcafc2a4              2  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_new.groupby('rfm_segments').agg(lambda x:x.value_counts().index[0])[['gender','age_group','rfm_score','clusters',\n",
    "                                                                             'offer','info_or_promo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfm_segments         info_or_promo\n",
       "ideal                2                 3839\n",
       "                     3                 3503\n",
       "                     1                  909\n",
       "                     0                  564\n",
       "need_to_reach_out    0                16307\n",
       "                     1                 4503\n",
       "                     2                 4016\n",
       "                     3                 3177\n",
       "not_spending_enough  0                 4548\n",
       "                     1                  755\n",
       "                     2                  128\n",
       "                     3                  119\n",
       "valuable             2                 7426\n",
       "                     3                 6788\n",
       "                     0                 3564\n",
       "                     1                 3142\n",
       "Name: info_or_promo, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rfm_segments         gender\n",
       "ideal                F          4506\n",
       "                     M          4173\n",
       "                     O           126\n",
       "need_to_reach_out    M         14465\n",
       "                     F          7429\n",
       "                     O           281\n",
       "not_spending_enough  M          2677\n",
       "                     F          1050\n",
       "                     O            50\n",
       "valuable             M         10330\n",
       "                     F          9823\n",
       "                     O           312\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rfm_segments         clusters\n",
       "ideal                1            6647\n",
       "                     2            2154\n",
       "                     0              14\n",
       "need_to_reach_out    3           14859\n",
       "                     0            6459\n",
       "                     2            4189\n",
       "                     1            2496\n",
       "not_spending_enough  3            3685\n",
       "                     0            1833\n",
       "                     2              32\n",
       "valuable             1           12289\n",
       "                     2            7233\n",
       "                     3             894\n",
       "                     0             504\n",
       "Name: clusters, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rfm_segments         offer                           \n",
       "ideal                fafdcd668e3743c1bb461111dcafc2a4    1089\n",
       "                     2298d6c36e964ae4a3e7e9706d1fb8c2    1002\n",
       "                     ae264e3637204a6fb9bb56bc8210ddfd     967\n",
       "                     f19421c1d4aa40978ebb69ca19b0e20d     936\n",
       "                     0b1e1539f2cc45b7b9fa7c272da2e1d7     929\n",
       "                     2906b810c7d4411798c6938adc9daaa5     908\n",
       "                     9b98b8c7a33c4b65b9aebfe6a799e6d9     907\n",
       "                     4d5c57ea9a6940dd891ad53e9dbe8da0     872\n",
       "                     3f207df678b143eea3cee63160fa8bed     611\n",
       "                     5a8bc65990b245e5a138643cd4eb9837     594\n",
       "need_to_reach_out    3f207df678b143eea3cee63160fa8bed    3132\n",
       "                     5a8bc65990b245e5a138643cd4eb9837    3094\n",
       "                     0b1e1539f2cc45b7b9fa7c272da2e1d7    2835\n",
       "                     4d5c57ea9a6940dd891ad53e9dbe8da0    2797\n",
       "                     ae264e3637204a6fb9bb56bc8210ddfd    2777\n",
       "                     2906b810c7d4411798c6938adc9daaa5    2738\n",
       "                     9b98b8c7a33c4b65b9aebfe6a799e6d9    2724\n",
       "                     f19421c1d4aa40978ebb69ca19b0e20d    2707\n",
       "                     2298d6c36e964ae4a3e7e9706d1fb8c2    2645\n",
       "                     fafdcd668e3743c1bb461111dcafc2a4    2554\n",
       "not_spending_enough  3f207df678b143eea3cee63160fa8bed     607\n",
       "                     0b1e1539f2cc45b7b9fa7c272da2e1d7     603\n",
       "                     2906b810c7d4411798c6938adc9daaa5     579\n",
       "                     9b98b8c7a33c4b65b9aebfe6a799e6d9     578\n",
       "                     5a8bc65990b245e5a138643cd4eb9837     570\n",
       "                     4d5c57ea9a6940dd891ad53e9dbe8da0     558\n",
       "                     f19421c1d4aa40978ebb69ca19b0e20d     529\n",
       "                     ae264e3637204a6fb9bb56bc8210ddfd     516\n",
       "                     2298d6c36e964ae4a3e7e9706d1fb8c2     509\n",
       "                     fafdcd668e3743c1bb461111dcafc2a4     501\n",
       "valuable             fafdcd668e3743c1bb461111dcafc2a4    2188\n",
       "                     2298d6c36e964ae4a3e7e9706d1fb8c2    2169\n",
       "                     9b98b8c7a33c4b65b9aebfe6a799e6d9    2146\n",
       "                     ae264e3637204a6fb9bb56bc8210ddfd    2114\n",
       "                     4d5c57ea9a6940dd891ad53e9dbe8da0    2103\n",
       "                     f19421c1d4aa40978ebb69ca19b0e20d    2090\n",
       "                     5a8bc65990b245e5a138643cd4eb9837    2062\n",
       "                     2906b810c7d4411798c6938adc9daaa5    2060\n",
       "                     0b1e1539f2cc45b7b9fa7c272da2e1d7    2007\n",
       "                     3f207df678b143eea3cee63160fa8bed    1981\n",
       "Name: offer, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(profile_new.groupby('rfm_segments').info_or_promo.value_counts())\n",
    "display(profile_new.groupby('rfm_segments').gender.value_counts())\n",
    "display(profile_new.groupby('rfm_segments').clusters.value_counts())\n",
    "display(profile_new.groupby('rfm_segments').offer.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ideal\" customer group:\n",
    "- The most popular offers are \"fafdcd668e3743c1bb461111dcafc2a4\" (discount) and \"2298d6c36e964ae4a3e7e9706d1fb8c2\" (discount).\n",
    "- The most popular offer type is \"discount\".\n",
    "- The most common cluster is \"1\".\n",
    "- About half of the customers are female and the other half are male.\n",
    "\n",
    "\n",
    "\"Valuable\" customer group:\n",
    "- The most popular offers are \"fafdcd668e3743c1bb461111dcafc2a4\" (discount) and \"2298d6c36e964ae4a3e7e9706d1fb8c2\" (discount).\n",
    "- The most popular offer type is \"discount\"\n",
    "- The most common cluster is \"1\".\n",
    "- About half of the customers are female and the other half are male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component conclusion\n",
    "\n",
    "Questions to answer:\n",
    "1. How does the customer groups labeled as ideal and valuable react to each type of offer?\n",
    "2. Should the marketing team send offer to a particular rewards program customer?\n",
    "3. Given the current state of the model, what improvements can be made in regards to modeling performance for each class label, to achieve a minimized cost after applying the cost assumptions?\n",
    "\n",
    "Findings:\n",
    "- Discount offers are the most popular ones among customers who are labeled as \"ideal\" and \"valuable\". In addition, the same groups of customers most likely come from cluster 1.\n",
    "- To answer the second question, our original intent was to train/tune a machine learning model and let it predict which offer types a given customer would most likely to complete. Then the business could plan ahead using these prediction results. However, even after taking the risk of information leakage by adding the \"channels\" column, our final model still produces inferior results. \n",
    "- Based on this condition and the learning objective (multi-class) of our model, I switched from conducting a cost analysis to a cost simulation, in order to answer question 3. As a result, we found the area that future improvements is most needed is class 3 results. Specifically, we would like to achieve a 0.77 for class 3 recall in order to minimize the cost, given all other class predictions does not change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
