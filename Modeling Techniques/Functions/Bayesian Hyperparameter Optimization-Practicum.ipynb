{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search space\n",
    "\n",
    "import time\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "lgb_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 3, 20, 1)),\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 500, 1)),\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 2, 100, 1)),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.6, 1.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 100.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 100.0),\n",
    "    'min_child_samples': scope.int(hp.quniform('min_child_samples', 2, 100, 5)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt(estimator, param_space, X_train, y_train, X_test, y_test, num_eval, eval_metric=None):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    def objective_function(params):\n",
    "        model = estimator(**params)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=3, scoring=eval_metric).mean()\n",
    "        return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "    \n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective_function, \n",
    "                      param_space, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate= np.random.RandomState(1))\n",
    "    loss = [x['result']['loss'] for x in trials.trials]\n",
    "    \n",
    "    \n",
    "    if str(estimator) == \"<class 'lightgbm.sklearn.LGBMRegressor'>\":\n",
    "        \n",
    "        for each in ['num_leaves','max_depth','n_estimators','min_child_samples']:\n",
    "            best_param[each] = int(best_param[each])\n",
    "        \n",
    "        model_best = estimator(**best_param)     \n",
    "        model_best.fit(X_train, y_train)\n",
    "        y_pred = model_best.predict(X_test)\n",
    "        \n",
    "    \n",
    "    if str(estimator) == \"<class 'xgboost.sklearn.XGBRegressor'>\":\n",
    "         \n",
    "        for each in ['n_estimators','max_depth','min_child_weight']:\n",
    "            best_param[each] = int(best_param[each])\n",
    "                \n",
    "        model_best = estimator(**best_param)     \n",
    "        model_best.fit(X_train, y_train)\n",
    "        y_pred = model_best.predict(X_test)\n",
    "        \n",
    "    \n",
    "    if str(estimator) == \"<class 'catboost.core.CatBoostRegressor'>\":\n",
    "         \n",
    "        for each in ['iterations','depth']:#,'min_child_samples','num_leaves']:\n",
    "            best_param[each] = int(best_param[each])\n",
    "                \n",
    "        model_best = estimator(**best_param)     \n",
    "        model_best.fit(X_train, y_train)\n",
    "        test_pool = Pool(X_test)\n",
    "        y_pred = model_best.predict(test_pool)\n",
    "\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"##### Results #####\")\n",
    "    print(\"Score best parameters: \", min(loss)*-1)\n",
    "    print(\"Best parameters: \", best_param)\n",
    "    print(\"Test Score: \", mean_absolute_error(y_test, y_pred))\n",
    "    print(\"Time elapsed: \", time.time() - start)\n",
    "    print(\"Parameter combinations evaluated: \", num_eval)\n",
    "    \n",
    "    \n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval=50\n",
    "eval_metric = 'neg_mean_absolute_error'\n",
    "lgb_hyperopt = hyperopt(LGBMRegressor, lgb_space, x_train_new, y_train, x_val_new, y_val, num_eval, eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the selected parameters\n",
    "\n",
    "unpack_function = lambda l: [item for sublist in l for item in sublist]\n",
    "unpack_all = lgb_hyperopt.best_trial['misc']['vals']\n",
    "unpack_values = [i for i in unpack_all.values()]\n",
    "values = unpack_function(unpack_values)\n",
    "keys = [i for i in unpack_all.keys()]\n",
    "best_param = {keys[i]: values[i] for i in range(len(keys))} \n",
    "\n",
    "for each in ['num_leaves','max_depth','n_estimators','min_child_samples']:\n",
    "    best_param[each] = int(best_param[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model performance assessment\n",
    "\n",
    "lgb = LGBMRegressor(**best_param)\n",
    "lgb.fit(x_train_new, y_train, eval_set = [(x_train_new, y_train), (x_val_new, y_val)],\n",
    "         eval_metric= 'mae', early_stopping_rounds=30, verbose=20)\n",
    "\n",
    "y_pred_final = lgb.predict(x_test_new)\n",
    "print('Final MAE from using LightGBM Regressor is: %.3f' % mean_absolute_error(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
