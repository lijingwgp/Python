{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "virtual-galaxy",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train-test split\n",
    "\n",
    "x = train\n",
    "y = train['target']\n",
    "del train['target']    #or train.remove('target')\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1,stratify=y)\n",
    "\n",
    "print (\"The train data has\", train.shape)\n",
    "print (\"The test data has\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-reducing",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(['Id'], axis=1, inplace=True)\n",
    "x_test.drop(['Id'], axis=1, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(x_train)\n",
    "x_train_new = scaler.transform(x_train)\n",
    "\n",
    "scaler.fit(x_test)\n",
    "x_test_new = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-finnish",
   "metadata": {},
   "source": [
    "### Cross-validated Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cross_val_score(ElasticNet(random_state=123), x_train, y_train, \n",
    "                           scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
    "\n",
    "print('Baseline MAE from using ElasticNet is: {.3f}'.format(baseline.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cross_validate(RandomForestRegressor(n_estimators=100, random_state=123), x_train, y_train, \n",
    "                          scoring='neg_mean_absolute_error', cv=5, n_jobs=-1, error_score='raise')\n",
    "\n",
    "baseline['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-lindsay",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGB as a potentially improved model\n",
    "\n",
    "lgb = LGBRegressor(random_state=123)\n",
    "lgb.fit(x_train, y_train, eval_set=[(x_val,y_val)], eval_metric='mae', early_stopping_rounds=30, verbose=20)\n",
    "y_pred = lgb.predict(x_val)\n",
    "\n",
    "print('Improved MAE from using LightGBM Regressor is: {.3f}'.format(mean_absolute_error(y_val, y_pred)))\n",
    "\n",
    "# or we can use the cv method\n",
    "\n",
    "dftrainLGB = lgb.Dataset(data = x_train, label = y_train, feature_name = x_train.columns.tolist())\n",
    "params = {'objective': 'regression'}\n",
    "lgb.cv(\n",
    "    params,\n",
    "    dftrainLGB,\n",
    "    num_boost_round=100,\n",
    "    nfold=3,\n",
    "    metrics='mae',\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
