model = XGBClassifier(silent=False,objective=‘binary:logistic’,n_estimators=400)
eval_set=[(X_train, y_train), (X_test, y_test)]
model.fit(X_train, y_train, eval_metric=[“auc”,“error”,“logloss”], eval_set=eval_set, verbose=True)

eval_set = [(X_train, y_train), (X_test, y_test)]
eval_metric = ["auc","error"]
%time model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=True)


param_grid = {
        'max_depth': [6, 10, 15, 20],
        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],
        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],
        'gamma': [0, 0.25, 0.5, 1.0],
        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],
        'n_estimators': [300]}

fit_params = {'eval_metric': 'mlogloss',
              'early_stopping_rounds': 30,
              'eval_set': [(X_test, y_test)]}

xgb_random = XGBClassifier(seed=seed, n_jobs=-1, objective='multi:softprob')
xgb_random_search = RandomizedSearchCV(xgb_random, param_grid, n_iter=5, verbose=1, cv=3, 
                                       fit_params=fit_params, scoring='f1_weighted', refit=False, random_state=seed)
